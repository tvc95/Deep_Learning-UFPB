{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 2\n",
    "\n",
    "## Implementação do Perceptron de Múltiplas Camadas\n",
    "\n",
    "Implemente uma rede perceptron de múltiplas camadas treinando-a com os seguintes\n",
    "algoritmos:\n",
    "\n",
    "    a) algoritmo da retropropagação em modo estocástico usando a regra delta;\n",
    "    b) algoritmo da retropropagação em modo por lote usando a regra delta;\n",
    "    c) algoritmo da retropropagação usando a regra delta com termo do momento;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot, shape, zeros, transpose, matrix\n",
    "from math import ceil, sinh, cosh\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes desenvolvidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer():\n",
    "    \"\"\"\n",
    "    Classe representando a camada de neurônios da rede\n",
    "    Parâmetros:\n",
    "        - num_neurons: o número de neurônios na camada\n",
    "        - num_inputs: o número de entradas que cada neurônio da camada terá\n",
    "    \"\"\"\n",
    "    def __init__(self, num_neurons, num_inputs):\n",
    "        self.weights = 2 * random.random((num_inputs, num_neurons)) - 1\n",
    "        \n",
    "class NeuralNetwork():\n",
    "    def __init__(self, layer1, layer2):\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "\n",
    "    def activation_func(self, func_name, z):\n",
    "        \"\"\"\n",
    "        Executa uma das funções de ativação da rede\n",
    "        Parâmetros:\n",
    "            - func_name: nome da função de ativação\n",
    "            - z: valor a ser aplicado na função de ativação\n",
    "        \"\"\"\n",
    "        if(func_name == 'sigmoid'):\n",
    "            return 1 / (1 + exp(-z))\n",
    "        elif(func_name == 'tanh'):\n",
    "            return sinh(z) / cosh(z)\n",
    "        elif(func_name == 'relu'):\n",
    "            if(z <= 0):\n",
    "                return 0\n",
    "            else:\n",
    "                return z\n",
    "        elif(func_name == 'sigmoid_deriv'):\n",
    "            return z * (1-z)\n",
    "        elif(func_name == 'relu_deriv'):\n",
    "            if(z<=0):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "    def train_network(self, inputs, expected_outs, num_iterations, learning_rate, backprop_type):\n",
    "        \"\"\"\n",
    "        Método para treinamento da rede neural e ajuste dos pesos sinápticos\n",
    "        Parâmetros:\n",
    "            - inputs: o conjunto de treinamento\n",
    "            - expected_outs: saídas desejadas do conjunto de treinamento\n",
    "            - num_iterations: épocas (duração do treinamento)\n",
    "            - learning_rate: taxa de aprendizado\n",
    "            - backprop_type: o tipo de retropropagação a ser usado no treinamento (\"momentum\", \"batch\", \"stochastic)\n",
    "            \n",
    "        \"\"\"\n",
    "        alpha = 0.1                                     #constante do momento (0 <= alpha < 1)\n",
    "        samples, features = shape(inputs)\n",
    "        layer1_adjustment = 0\n",
    "        layer2_adjustment = 0\n",
    "        \n",
    "        for count in range(num_iterations):\n",
    "            #print(\"época \" + str(count + 1) + \":\")\n",
    "\n",
    "            if((backprop_type == \"batch\") or (backprop_type == \"momentum\")):\n",
    "                out_l1, out_l2 = self.classify(inputs)\n",
    "                \n",
    "                ### Cálculo dos erros na camada 2\n",
    "                layer2_error = expected_outs - out_l2\n",
    "                layer2_grad = layer2_error * self.activation_func(\"sigmoid_deriv\",out_l2)      # gradiente local do neurônio da camada de saída\n",
    "                #print(\"erro camada 2: \\n\" + str(layer2_error))\n",
    "\n",
    "                ## Com base no resultado da camada 2, calcular o erro na camada 1\n",
    "                layer1_error = layer2_grad.dot(self.layer2.weights.T)\n",
    "                layer1_grad = layer1_error * self.activation_func(\"sigmoid_deriv\",out_l1)                 # gradiente da camada 1\n",
    "                #print(\"erro camada 1: \\n\" + str(layer1_error))\n",
    "                \n",
    "                if(backprop_type == \"momentum\"):\n",
    "                    if(count == 0):\n",
    "                        layer1_adjustment = (inputs.T.dot(layer1_grad)) * learning_rate\n",
    "                        layer2_adjustment = out_l1.T.dot(layer2_grad) * learning_rate\n",
    "                    else:\n",
    "                        ## Cálculo da regra delta com constante do momento\n",
    "                        layer1_adjustment = (alpha * temp1) + ((inputs.T.dot(layer1_grad)) * learning_rate)\n",
    "                        layer2_adjustment = (alpha * temp2) +(out_l1.T.dot(layer2_grad) * learning_rate)\n",
    "                else:\n",
    "                    ## Cálculo normal da regra delta\n",
    "                    layer1_adjustment = (inputs.T.dot(layer1_grad)) * learning_rate\n",
    "                    layer2_adjustment = out_l1.T.dot(layer2_grad) * learning_rate\n",
    "\n",
    "                self.layer1.weights += layer1_adjustment\n",
    "                temp1 = self.layer1.weights\n",
    "                self.layer2.weights += layer2_adjustment\n",
    "                temp2 = self.layer2.weights\n",
    "            else:\n",
    "                #treinamento estocástico\n",
    "                for count2 in range(samples):\n",
    "                    out_l1, out_l2 = self.classify(inputs[count2, :])\n",
    "                    #print(\"saída camada 1: \\n\" + str(out_l1) + \"\\nsaída camada 2: \\n\" + str(out_l2))\n",
    "\n",
    "                    ### Cálculo dos erros na camada 2\n",
    "                    layer2_error = expected_outs[count2, :] - out_l2\n",
    "                    layer2_grad = layer2_error * self.activation_func(\"sigmoid_deriv\",out_l2)                 # gradiente local do neurônio da camada de saída\n",
    "\n",
    "                    ## Com base no resultado da camada 2, calcular o erro na camada 1\n",
    "                    layer1_error = layer2_grad.dot(self.layer2.weights.T)\n",
    "                    layer1_grad = layer1_error * self.activation_func(\"sigmoid_deriv\",out_l1)                 # gradiente da camada 1\n",
    "\n",
    "                    ## Regra delta\n",
    "                    ar = array([inputs[count2, :]])\n",
    "                    l1_grad = array([layer1_grad])\n",
    "                    o_l1 = array([out_l1])\n",
    "                    l2_grad = array([out_l2])\n",
    "                    \n",
    "                    layer1_adjustment = (ar.T.dot(l1_grad)) * learning_rate\n",
    "                    layer2_adjustment = o_l1.T.dot(l2_grad) * learning_rate\n",
    "\n",
    "                    self.layer1.weights += layer1_adjustment\n",
    "                    temp1 = self.layer1.weights\n",
    "                    self.layer2.weights += layer2_adjustment\n",
    "                    temp2 = self.layer2.weights\n",
    "\n",
    "    def classify(self, inputs):\n",
    "        \"\"\"\n",
    "        Classifica uma entrada pela rede neural\n",
    "        Parâmetros:\n",
    "            - inputs: valor de um conjunto de treinamento/classificação\n",
    "        \"\"\"\n",
    "        output_from_layer1 = self.activation_func(\"sigmoid\", (dot(inputs, self.layer1.weights)))\n",
    "        output_from_layer2 = self.activation_func(\"sigmoid\", (dot(output_from_layer1, self.layer2.weights)))\n",
    "        return output_from_layer1, output_from_layer2\n",
    "\n",
    "    def print_weights(self):\n",
    "        print(\"Pesos da camada 1: \")\n",
    "        print (self.layer1.weights)\n",
    "        print(\"Pesos da camada 2: \")\n",
    "        print (self.layer2.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informações adicionais\n",
    "\n",
    "Por questões de simplicidade, foi decidido a implementação de um perceptron com apenas duas camadas. Como pode ser visto no código abaixo, são criadas duas camadas de neurônios, sendo a primeira com 5 neurônios, recebendo as entradas do conjunto de treinamento, e a segunda contendo apenas 1 neurônio, recebendo como entrada as saídas dos neurônios da camada anterior.\n",
    "\n",
    "O conjunto de treinamento contém 6 exemplos de entrada e 2 exemplos utilizados para treinar a classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos antes de iniciar o treinamento: \n",
      "Pesos da camada 1: \n",
      "[[-0.96044121  0.54458214 -0.34804275 -0.90906084 -0.27221613]\n",
      " [ 0.02592391  0.31021019 -0.47048923  0.30738122  0.77587053]\n",
      " [ 0.20432253  0.20984768 -0.53915093  0.77970849  0.01007366]]\n",
      "Pesos da camada 2: \n",
      "[[ 0.1281504 ]\n",
      " [ 0.29396554]\n",
      " [ 0.69110499]\n",
      " [-0.33849427]\n",
      " [ 0.06032128]]\n",
      "Pesos de saída (após última iteração do treinamento): \n",
      "Pesos da camada 1: \n",
      "[[-8.04447826e+206  4.71504678e+206 -2.21195420e+206 -7.52860963e+206\n",
      "  -2.27625560e+206]\n",
      " [-2.68084247e+205  1.86870369e+206 -5.25342650e+206  3.78318035e+206\n",
      "   6.45255406e+206]\n",
      " [ 2.07611264e+206  2.17763540e+206 -3.90018438e+206  6.54853242e+206\n",
      "   7.70100871e+204]]\n",
      "Pesos da camada 2: \n",
      "[[ 2.20160601e+206]\n",
      " [ 2.76595425e+206]\n",
      " [ 6.99586373e+206]\n",
      " [-3.12483198e+206]\n",
      " [-1.08295430e+206]]\n",
      "Saídas obtidas com relação aos conjuntos treinados: \n",
      "[1.] [1.] [0.] [1.] [0.] [1.]\n",
      "Teste com entradas não treinadas: \n",
      "[1, 1, 0] ==>[1.]\n",
      "[0, 0, 1] ==> [1.]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    LEARNING_RATE = 0.5\n",
    "\n",
    "    #random.seed(1)\n",
    "\n",
    "    # Rede neural de duas camadas (setando valores (numero de neurônios e seu numero de entradas))\n",
    "    layer1 = NeuronLayer(5, 3)\n",
    "    layer2 = NeuronLayer(1, 5)\n",
    "\n",
    "    neural_network = NeuralNetwork(layer1, layer2)\n",
    "\n",
    "    # Atribuição randomica de pesos a rede neural\n",
    "    print (\"Pesos antes de iniciar o treinamento: \")\n",
    "    neural_network.print_weights()\n",
    "\n",
    "    # Conjunto de treinamento (6 exemplos com 3 valores de entrada e 1 valor de saída).\n",
    "    training_set_inputs = array([[0, 1, 1], [1, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1], [0, 0, 0]])\n",
    "    training_set_outputs = array([[1, 1, 0, 1, 0, 1]]).T\n",
    "\n",
    "    # treinamento da rede neural\n",
    "    neural_network.train_network(training_set_inputs, training_set_outputs, 5000, LEARNING_RATE, \"momentum\")\n",
    "\n",
    "    print (\"Pesos de saída (após última iteração do treinamento): \")\n",
    "    neural_network.print_weights()\n",
    "\n",
    "    print(\"Saídas obtidas com relação aos conjuntos treinados: \")\n",
    "    h, out3 = neural_network.classify(array([0, 1, 1]))\n",
    "    h, out4 = neural_network.classify(array([1, 0, 1]))\n",
    "    h, out5 = neural_network.classify(array([0, 1, 0]))\n",
    "    h, out6 = neural_network.classify(array([1, 0, 0]))\n",
    "    h, out7 = neural_network.classify(array([1, 1, 1]))\n",
    "    h, out8 = neural_network.classify(array([0, 0, 0]))\n",
    "    print(str(out3) + \" \" + str(out4) + \" \" + str(out5) + \" \" + str(out6) + \" \" + str(out7) + \" \" + str(out8))\n",
    "\n",
    "    print (\"Teste com entradas não treinadas: \")\n",
    "    h, output = neural_network.classify(array([1, 1, 0]))\n",
    "    print(\"[1, 1, 0] ==>\" + str(output))\n",
    "    h, out2 = neural_network.classify(array([0, 0, 1]))\n",
    "    print (\"[0, 0, 1] ==> \" + str(out2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplos de saídas\n",
    "\n",
    "Considerando a entrada \n",
    "    x = [0, 0, 1], [0, 1, 1], [1, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1], [0, 0, 0]]\n",
    "\n",
    "e a saída esperada\n",
    "    y = [1, 1, 1, 0, 1, 0, 1]\n",
    "\n",
    "\n",
    "### Regra delta com termo do momento (alpha = 0.1)\n",
    "\n",
    "\n",
    "##### Pesos antes de iniciar o treinamento:\n",
    "\n",
    "Pesos da camada 1:\n",
    "\n",
    "    [[-0.83475838  0.01699496 -0.42069627 -0.29071839  0.04908799]\n",
    "     [ 0.35578725  0.95919087  0.93309106  0.1644563  -0.7621909 ]\n",
    "     [-0.47051837  0.33232453  0.09078856 -0.21768878  0.30531074]]\n",
    " \n",
    "Pesos da camada 2: \n",
    "\n",
    "    [[ 0.98271422]\n",
    "     [-0.50859378]\n",
    "     [-0.32569893]\n",
    "     [ 0.85009907]\n",
    "     [ 0.60985323]]\n",
    " \n",
    "##### Pesos de saída (após última iteração do treinamento): \n",
    "\n",
    "Pesos da camada 1: \n",
    "\n",
    "    [[-6.79963532e+206 -5.53665695e+205 -3.76593325e+206 -1.99586382e+206\n",
    "       8.52225547e+205]\n",
    "     [ 2.29986397e+206  8.62115036e+206  8.33225912e+206 -5.31630268e+205\n",
    "      -6.78011909e+206]\n",
    "     [-1.52776764e+206  2.27610798e+206  5.12704297e+205  6.02530888e+205\n",
    "       3.72729736e+206]]\n",
    "   \n",
    "Pesos da camada 2: \n",
    "\n",
    "    [[ 7.10638320e+206]\n",
    "     [-4.96690861e+206]\n",
    "     [-3.98957106e+206]\n",
    "     [ 7.17854279e+206]\n",
    "     [ 7.50448908e+206]]\n",
    " \n",
    "Saídas obtidas com relação aos conjuntos treinados:\n",
    "\n",
    "    [1.] [1.] [0.] [1.] [0.] [1.]\n",
    "    \n",
    "Teste com entradas não treinadas: \n",
    "\n",
    "    [1, 1, 0] ==>[0.]\n",
    "    [0, 0, 1] ==> [1.]\n",
    "    \n",
    "    \n",
    "### Backpropagation por lotes\n",
    "\n",
    "##### Pesos antes de iniciar o treinamento:\n",
    "\n",
    "Pesos da camada 1:\n",
    "\n",
    "    [[ 0.99201534 -0.41928641 -0.36273008 -0.56463967 -0.33377834]\n",
    "     [ 0.00518027  0.33828494 -0.93426758 -0.59924854 -0.39466139]\n",
    "     [-0.06669957  0.89812476  0.71153876 -0.59760856 -0.86776041]]\n",
    " \n",
    "Pesos da camada 2: \n",
    "\n",
    "    [[ 0.59587405]\n",
    "     [-0.25582945]\n",
    "     [ 0.88503957]\n",
    "     [-0.38695687]\n",
    "     [-0.37647607]]\n",
    " \n",
    "##### Pesos de saída (após última iteração do treinamento): \n",
    "\n",
    "Pesos da camada 1: \n",
    "\n",
    "    [[-0.48659945 -0.33860451  0.66946286 -0.87395413  0.6350526 ]\n",
    "     [ 0.56707141 -0.77239012 -0.61720593  0.50100892  0.24499828]\n",
    "     [ 0.3634765  -0.04300949 -0.15063007 -0.45175999  0.27899597]]\n",
    "   \n",
    "Pesos da camada 2: \n",
    "\n",
    "    [[ 4.46449034]\n",
    "     [-5.26312244]\n",
    "     [ 9.41891081]\n",
    "     [ 1.98700465]\n",
    "     [ 0.28427291]]\n",
    " \n",
    "Saídas obtidas com relação aos conjuntos treinados (valores brutos, sem arredondamento):\n",
    "\n",
    "    [0.97816022] [0.99820302] [0.01898581] [0.9848934] [0.01930171] [0.99570405]\n",
    "    \n",
    "Teste com entradas não treinadas:\n",
    "\n",
    "    [1, 1, 0] ==>[0.02865417]\n",
    "    [0, 0, 1] ==> [0.99925982]\n",
    "    \n",
    "    \n",
    "### Backpropagation estocástico\n",
    "\n",
    "##### Pesos antes de iniciar o treinamento:\n",
    "\n",
    "Pesos da camada 1:\n",
    "\n",
    "    [[-0.48248758 -0.33676161  0.67495536 -0.87249905  0.63841294]\n",
    "     [ 0.58824616 -0.76286918 -0.59459496  0.50216722  0.26171083]\n",
    "     [ 0.36356913 -0.042537   -0.15160516 -0.44412406  0.28095638]]\n",
    " \n",
    "Pesos da camada 2: \n",
    "\n",
    "    [[ 0.66638244]\n",
    "     [ 0.23120041]\n",
    "     [ 0.90145531]\n",
    "     [-0.65765234]\n",
    "     [ 0.23953875]]\n",
    " \n",
    "##### Pesos de saída (após última iteração do treinamento): \n",
    "\n",
    "Pesos da camada 1: \n",
    "\n",
    "    [[ 1.53124017 -2.07888298 -5.02242377 -0.16824009 -0.31320068]\n",
    "     [-3.52113783  3.7862728  -2.65729379 -1.59201911 -0.31439086]\n",
    "     [-0.94128309  1.73251232  5.00029858 -0.01472485 -0.84990957]]\n",
    "   \n",
    "Pesos da camada 2: \n",
    "\n",
    "    [[8286.4585554 ]\n",
    "     [5460.94214015]\n",
    "     [7328.32627216]\n",
    "     [6109.32059047]\n",
    "     [9544.06519478]]\n",
    " \n",
    "Saídas obtidas com relação aos conjuntos treinados:\n",
    "\n",
    "    [1.] [1.] [1.] [1.] [1.] [1.]\n",
    "    \n",
    "Teste com entradas não treinadas:\n",
    "\n",
    "    [1, 1, 0] ==> [1.]\n",
    "    [0, 0, 1] ==> [1.]\n",
    "    \n",
    "###### OBS.: Para as saídas no modo estocástico, o erro identificado está no gradiente da 1a camada, que diminui até se tornar 0 em certo ponto, influindo na saída e classificação errada das entradas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
